{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c5_HHutGM75"
      },
      "source": [
        "# Task A: Multi-Class Classification\n",
        "\n",
        "Gravitational lensing has been a cornerstone in many cosmology experiments and studies since it was discussed in Einsteinâ€™s calculations back in 1936 and discovered in 1979, and one area of particular interest is the study of dark matter via substructure in strong lensing images. In this challenge, we focus on exploring the potential of supervised models in identifying dark matter based on simulated strong lensing images with different substructure.\n",
        "\n",
        "This is an example notebook for the Multi-Class Classification Challenge. In this notebook, we demonstrate a simple CNN model implemented using the PyTorch library to solve the task of multi-class classification of strong lensing images.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The Dataset consists of three classes, strong lensing images with no substructure, CDM (cold dark matter) substructure, and axion substructure. The images have been normalized using min-max normalization, but you are free to use any normalization or data augmentation methods to improve your results.\n",
        "\n",
        "Link to the Dataset: https://drive.google.com/file/d/1AZAJzJdm6FJT4rIyY9N_6FcKLf8VZtn8/view?usp=sharing\n",
        "\n",
        "### Evaluation Metrics\n",
        "\n",
        "* ROC curve (Receiver Operating Characteristic curve) and AUC score (Area Under the ROC Curve)   \n",
        "* Accuracy, Precision, Recall, and F1 Score\n",
        "\n",
        "The model performance will be tested on the hidden test dataset based on the above metrics. More details about these metrics and the code to calculate them has been shared below.\n",
        "\n",
        "### Instructions for using the notebook\n",
        "\n",
        "1. Use GPU acceleration: (Edit --> Notebook settings --> Hardware accelerator --> GPU)\n",
        "2. Run the cells: (Runtime --> Run all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIiqNE2UGLev"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Check if the dataset folder is missing\n",
        "if not os.path.exists('./dataset'):\n",
        "    # Download and extract the dataset\n",
        "    !gdown \"http://drive.google.com/uc?id=1AZAJzJdm6FJT4rIyY9N_6FcKLf8VZtn8\"\n",
        "    !unzip -q dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPMhi3uvHTBK"
      },
      "source": [
        "## Multi-Class Classification using a Supervised Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513RtRF4Hf3Z"
      },
      "source": [
        "### 1. Data Visualization and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR-nMgqoIary"
      },
      "source": [
        "#### 1.1 Import all the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "agXdpFwPPiHw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "from numpy import interp\n",
        "from itertools import cycle\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mw4NndbHsiY"
      },
      "source": [
        "#### 1.2 Preview the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0m86HY9DJSl"
      },
      "outputs": [],
      "source": [
        "# Define the input paths\n",
        "train_path1 = './dataset/train/no'\n",
        "train_files1 = [os.path.join(train_path1, f) for f in os.listdir(train_path1) if f.endswith(\".npy\")]\n",
        "train_path2 = './dataset/train/cdm'\n",
        "train_files2 = [os.path.join(train_path2, f) for f in os.listdir(train_path2) if f.endswith(\".npy\")]\n",
        "train_path3 = './dataset/train/axion'\n",
        "train_files3 = [os.path.join(train_path3, f) for f in os.listdir(train_path3) if f.endswith(\".npy\")]\n",
        "\n",
        "# Number of samples to display per class\n",
        "n = 5\n",
        "\n",
        "# Plot the samples with no substructure\n",
        "i = 1\n",
        "print('Samples with no substructure: ')\n",
        "plt.rcParams['figure.figsize'] = [14, 14]  # Set the figure size\n",
        "for image in train_files1[:n]:\n",
        "    ax = plt.subplot(3, n, i)  # Create subplot\n",
        "    plt.imshow(np.load(image).reshape(64, 64), cmap='gray')  # Load and display the image\n",
        "    ax.get_xaxis().set_visible(False)  # Hide x-axis\n",
        "    ax.get_yaxis().set_visible(False)  # Hide y-axis\n",
        "    i += 1\n",
        "plt.show()  # Show the plot\n",
        "\n",
        "# Plot the samples with spherical substructure\n",
        "print('Samples with spherical substructure: ')\n",
        "plt.rcParams['figure.figsize'] = [14, 14]  # Set the figure size\n",
        "for image in train_files2[:n]:\n",
        "    ax = plt.subplot(3, n, i)  # Create subplot\n",
        "    plt.imshow(np.load(image).reshape(64, 64), cmap='gray')  # Load and display the image\n",
        "    ax.get_xaxis().set_visible(False)  # Hide x-axis\n",
        "    ax.get_yaxis().set_visible(False)  # Hide y-axis\n",
        "    i += 1\n",
        "plt.show()  # Show the plot\n",
        "\n",
        "# Plot the samples with vortex substructure\n",
        "print('Samples with vortex substructure: ')\n",
        "plt.rcParams['figure.figsize'] = [14, 14]  # Set the figure size\n",
        "for image in train_files3[:n]:\n",
        "    ax = plt.subplot(3, n, i)  # Create subplot\n",
        "    plt.imshow(np.load(image).reshape(64, 64), cmap='gray')  # Load and display the image\n",
        "    ax.get_xaxis().set_visible(False)  # Hide x-axis\n",
        "    ax.get_yaxis().set_visible(False)  # Hide y-axis\n",
        "    i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYWK0ZkMIMSr"
      },
      "source": [
        "#### 1.3 Import Training and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yrkV-fO9mWI"
      },
      "outputs": [],
      "source": [
        "# Set Batch Size\n",
        "batch_size = 100\n",
        "\n",
        "# Define a function to load .npy files\n",
        "def npy_loader(path):\n",
        "    sample = torch.from_numpy(np.load(path))  # Load the numpy file and convert it to a torch tensor\n",
        "    return sample\n",
        "\n",
        "# Load training data\n",
        "train_data = torchvision.datasets.DatasetFolder(root='./dataset/train', loader=npy_loader, extensions='.npy')\n",
        "print(\"Training Classes: \" + str(train_data.class_to_idx))  # Print the classes found in the training data\n",
        "train_data_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)  # Create a data loader for training data\n",
        "\n",
        "# Load validation data\n",
        "val_data = torchvision.datasets.DatasetFolder(root='./dataset/val', loader=npy_loader, extensions='.npy')\n",
        "print(\"Validation Classes: \" + str(val_data.class_to_idx))  # Print the classes found in the validation data\n",
        "val_data_loader = data.DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4)  # Create a data loader for validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABBx_F7oI_vC"
      },
      "source": [
        "### 2. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clDeWqesKU_8"
      },
      "source": [
        "#### 2.1 Defining a CNN Model\n",
        "\n",
        "You may refer to this [article](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148) to learn about Convolutional Neural Networks (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlDjFP-mraJG"
      },
      "outputs": [],
      "source": [
        "# Define the Convolutional Neural Network (CNN) class\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        # Define the first convolutional layer\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, stride=2, padding=0)\n",
        "        \n",
        "        # Define the second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0)\n",
        "        \n",
        "        # Define the third convolutional layer\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=3, stride=1, padding=0)\n",
        "        \n",
        "        # Define the first fully connected (linear) layer\n",
        "        self.linear1 = nn.Linear(120, 64)\n",
        "        \n",
        "        # Define the second fully connected (linear) layer\n",
        "        self.linear2 = nn.Linear(64, 3)\n",
        "        \n",
        "        # Define the activation function\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "        # Define the average pooling layer\n",
        "        self.avgpool = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the first convolutional layer\n",
        "        x = self.conv1(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        # Apply the second convolutional layer\n",
        "        x = self.conv2(x)\n",
        "        x = self.tanh(x)\n",
        "        x = self.avgpool(x)\n",
        "        \n",
        "        # Apply the third convolutional layer\n",
        "        x = self.conv3(x)\n",
        "        x = self.tanh(x)\n",
        "        \n",
        "        # Flatten the tensor for the fully connected layers\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        \n",
        "        # Apply the first fully connected layer\n",
        "        x = self.linear1(x)\n",
        "        x = self.tanh(x)\n",
        "        \n",
        "        # Apply the second fully connected layer\n",
        "        x = self.linear2(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the CNN model and move it to the appropriate device\n",
        "model = CNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ixy2T3PKpes"
      },
      "source": [
        "#### 2.2 Training the CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dsVNm2J7K67"
      },
      "outputs": [],
      "source": [
        "# Loss Function\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "\n",
        "# Calculate the number of batches for training data\n",
        "n_batches_train = (len(train_files1) * 3) / batch_size  # Equal number of files in each class\n",
        "\n",
        "# Set the number of training epochs\n",
        "n_epochs = 20\n",
        "loss_array = []  # To store the loss values\n",
        "\n",
        "# Progress bar for epochs\n",
        "pbar = tqdm(range(1, n_epochs + 1))\n",
        "for epoch in pbar:\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "\n",
        "    # Iterate over the training data loader\n",
        "    for step, (x_tr, y_tr) in enumerate(train_data_loader):\n",
        "        data = x_tr.to(device).float()  # Move input data to the device and convert to float\n",
        "        labels = y_tr.to(device, dtype=torch.long)  # Move labels to the device and convert to long\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "        outputs = model(data)  # Forward pass through the model\n",
        "        _, preds = torch.max(outputs.data, 1)  # Get the predictions\n",
        "        correct = (preds == labels).float().sum()  # Calculate the number of correct predictions\n",
        "        loss = criteria(outputs, labels)  # Calculate the loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update the model parameters\n",
        "\n",
        "        train_loss += loss.item()  # Accumulate the loss\n",
        "        train_acc += correct.item() / data.shape[0]  # Accumulate the accuracy\n",
        "\n",
        "    # Calculate the average loss and accuracy for the epoch\n",
        "    train_loss = train_loss / n_batches_train\n",
        "    train_acc = train_acc / n_batches_train\n",
        "    \n",
        "    # Display the training statistics\n",
        "    pbar.set_postfix({'Training Loss': train_loss, 'Training Acc': train_acc})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s73f62tkLopN"
      },
      "source": [
        "### 3. Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPIMrzmAMpsv"
      },
      "source": [
        "#### 3.1 Testing the CNN Model on Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emtSHxk6xsOG"
      },
      "outputs": [],
      "source": [
        "# Initialize lists to store scores and labels\n",
        "y_score = []\n",
        "y_test = []\n",
        "\n",
        "# Iterate over the validation data loader\n",
        "for _, (x_ts, y_ts) in enumerate(val_data_loader):\n",
        "    mini_val_data = x_ts.to(device).float()  # Move validation data to the device and convert to float\n",
        "    y_ts = y_ts.to(device, dtype=torch.long)  # Move labels to the device and convert to long\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for validation\n",
        "        outputs = model(mini_val_data)  # Forward pass through the model\n",
        "        probabilities = torch.nn.functional.softmax(outputs, dim=1)  # Apply softmax to get probabilities\n",
        "\n",
        "    # Append the probabilities and labels to the respective lists\n",
        "    y_score.append(probabilities.cpu().detach().numpy())\n",
        "    y_test.append(y_ts.cpu().detach().numpy())\n",
        "\n",
        "# Convert the lists to numpy arrays and reshape them\n",
        "y_score = np.asarray(y_score).reshape(-1, 3)\n",
        "y_val = np.asarray(y_test).reshape(-1)\n",
        "\n",
        "# Binarize the labels for multi-class evaluation\n",
        "y_val = label_binarize(y_val, classes=[0, 1, 2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4W2zGiNMtgj"
      },
      "source": [
        "#### 3.2 Plotting the ROC Curve\n",
        "\n",
        "You may refer to this [article](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) to learn about the ROC Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8CW4sWvMnLe"
      },
      "outputs": [],
      "source": [
        "# Number of classes\n",
        "n_classes = y_val.shape[1]\n",
        "\n",
        "# Initialize dictionaries to store false positive rates (fpr), true positive rates (tpr), and ROC AUC values\n",
        "fpr = dict()\n",
        "tpr = dict()\n",
        "roc_auc = dict()\n",
        "\n",
        "# Compute ROC curve and ROC area for each class\n",
        "for i in range(n_classes):\n",
        "    fpr[i], tpr[i], _ = roc_curve(y_val[:, i], y_score[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Compute micro-average ROC curve and ROC area\n",
        "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_val.ravel(), y_score.ravel())\n",
        "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "# Aggregate all false positive rates\n",
        "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "\n",
        "# Initialize mean true positive rate (tpr)\n",
        "mean_tpr = np.zeros_like(all_fpr)\n",
        "for i in range(n_classes):\n",
        "    mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "\n",
        "# Average it and compute macro-average ROC curve and ROC area\n",
        "mean_tpr /= n_classes\n",
        "fpr[\"macro\"] = all_fpr\n",
        "tpr[\"macro\"] = mean_tpr\n",
        "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "\n",
        "# Plotting the ROC curves\n",
        "plt.rcParams['figure.figsize'] = [7, 5]  # Set figure size\n",
        "lw = 2  # Line width\n",
        "plt.figure()\n",
        "\n",
        "# Plot micro-average ROC curve\n",
        "plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n",
        "         label='micro-average (area = {})'\n",
        "               ''.format(round(roc_auc[\"micro\"], 5)),\n",
        "         color='deeppink', linestyle=':', linewidth=4)\n",
        "\n",
        "# Plot macro-average ROC curve\n",
        "plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n",
        "         label='macro-average (area = {})'\n",
        "               ''.format(round(roc_auc[\"macro\"], 5)),\n",
        "         color='navy', linestyle=':', linewidth=4)\n",
        "\n",
        "# Plot ROC curves for each class\n",
        "labels = ['axion', 'cdm', 'no']\n",
        "colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])\n",
        "for i, color in zip(range(n_classes), colors):\n",
        "    plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n",
        "             label='{} (area = {})'\n",
        "             ''.format(labels[i], round(roc_auc[i], 5)))\n",
        "\n",
        "# Plot the diagonal line (chance level)\n",
        "plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc=\"lower right\", prop={\"size\": 10})  # Add legend\n",
        "plt.show()  # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.3 Calculate evaluation metrics\n",
        "\n",
        "You may refer to this [article](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9) to learn more about these metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert the predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_score, axis=1)\n",
        "\n",
        "# Convert y_val from one-hot encoding to class labels\n",
        "y_true = np.argmax(y_val, axis=1)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "\n",
        "# Calculate precision\n",
        "precision = precision_score(y_true, y_pred, average='weighted')\n",
        "print(f'Precision: {precision}')\n",
        "\n",
        "# Calculate recall\n",
        "recall = recall_score(y_true, y_pred, average='weighted')\n",
        "print(f'Recall: {recall}')\n",
        "\n",
        "# Calculate F1 score\n",
        "f1 = f1_score(y_true, y_pred, average='weighted')\n",
        "print(f'F1 Score: {f1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHD8VEzyOuad"
      },
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Fill out the pre- and post- hackathon surveys.\n",
        "* You are required to submit a Google Colab Jupyter Notebook (.ipynb and pdf) clearly showing your implementation along with the evaluation metrics (ROC curve, AUC score, and other metrics) for the validation data.\n",
        "* You must also submit the final trained model, including the model architecture and the trained weights ( For example: HDF5 file, .pb file, .pt file, etc. )\n",
        "* You can use this example notebook as a template for your work.\n",
        "\n",
        "> **_NOTE:_**  You are free to use any ML framework such as PyTorch, Keras, TensorFlow, etc."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "Hackathon_Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
