{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c5_HHutGM75"
      },
      "source": [
        "# Task B: Image Super-Resolution\n",
        "\n",
        "Gravitational lensing has been a cornerstone in many cosmology experiments and studies since it was discussed in Einsteinâ€™s calculations back in 1936 and discovered in 1979, and one area of particular interest is the study of dark matter via substructure in strong lensing images. In this challenge, we focus on exploring the potential of ML models in enhancing the resolution of lensing images.\n",
        "\n",
        "In this task, we will develop and train a super-resolution model to enhance the quality of low-resolution strong gravitational lensing images. The goal is to upscale noisy and blurry images to higher resolutions, improving their clarity and detail. Participants can explore different super-resolution techniques, including convolutional neural networks (CNNs), generative adversarial networks (GANs), and other deep learning approaches.\n",
        "\n",
        "![HR and LR Image Pair](https://github.com/pranath-reddy/DeepLearnHackathon/blob/main/GravitationalLensingChallenge/hr_lr_pair.png?raw=true)\n",
        "\n",
        "This is an example notebook for the Image Super-Resolution Challenge. In this notebook, we demonstrate a simple CNN model implemented using the PyTorch library to solve the task of super-resolution of strong lensing images.\n",
        "\n",
        "### Dataset\n",
        "\n",
        "The Dataset consists of high-resolution (HR) and low-resolution (LR) pairs. The images have been normalized using min-max normalization.\n",
        "\n",
        "Link to the Dataset: https://drive.google.com/file/d/1yJBvKD4saonRfSy4r0ceuD9qzgrdrHld/view?usp=sharing\n",
        "\n",
        "### Evaluation Metrics\n",
        "\n",
        "* MSE (Mean Squared error), SSIM (Similarity Index), and PSNR (Signal to Noise Ratio)   \n",
        "\n",
        "The model performance will be tested on the hidden test dataset based on the above metrics. More details about these metrics and the code to calculate them has been shared below.\n",
        "\n",
        "### Instructions for using the notebook\n",
        "\n",
        "1. Use GPU acceleration: (Edit --> Notebook settings --> Hardware accelerator --> GPU)\n",
        "2. Run the cells: (Runtime --> Run all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIiqNE2UGLev"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Check if the dataset folder is missing\n",
        "if not os.path.exists('./dataset_superres'):\n",
        "    # Download and extract the dataset\n",
        "    !gdown \"http://drive.google.com/uc?id=1yJBvKD4saonRfSy4r0ceuD9qzgrdrHld\"\n",
        "    !unzip -q dataset.zip\n",
        "    !mv dataset dataset_superres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPMhi3uvHTBK"
      },
      "source": [
        "## Single Image Super Resolution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "513RtRF4Hf3Z"
      },
      "source": [
        "### 1. Data Visualization and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR-nMgqoIary"
      },
      "source": [
        "#### 1.1 Import all the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "agXdpFwPPiHw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.utils.data as data\n",
        "import torch.nn.functional as F\n",
        "from skimage.metrics import structural_similarity as ssim, peak_signal_noise_ratio as psnr\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mw4NndbHsiY"
      },
      "source": [
        "#### 1.2 Preview the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0m86HY9DJSl"
      },
      "outputs": [],
      "source": [
        "# Define the input paths for high-resolution (HR) and low-resolution (LR) training images\n",
        "train_hr_path = './dataset_superres/train/HR'\n",
        "train_hr_files = [os.path.join(train_hr_path, f) for f in os.listdir(train_hr_path) if f.endswith(\".npy\")]\n",
        "train_lr_path = './dataset_superres/train/LR'\n",
        "train_lr_files = [os.path.join(train_lr_path, f) for f in os.listdir(train_lr_path) if f.endswith(\".npy\")]\n",
        "\n",
        "# Number of samples to display\n",
        "n = 5\n",
        "\n",
        "# Plot the high-resolution (HR) samples\n",
        "i = 1\n",
        "print('High-Resolution (HR) samples: ')\n",
        "plt.rcParams['figure.figsize'] = [14, 14]\n",
        "for image in train_hr_files[:n]:  # Loop through the first n HR images\n",
        "    ax = plt.subplot(2, n, i)  # Create subplot for the current image\n",
        "    plt.imshow(np.load(image).reshape(128,128), cmap='gray')  # Load and display the image in grayscale\n",
        "    ax.get_xaxis().set_visible(False)  # Hide x-axis\n",
        "    ax.get_yaxis().set_visible(False)  # Hide y-axis\n",
        "    i += 1  # Increment the subplot index\n",
        "plt.show()  # Display the plot\n",
        "\n",
        "# Plot the low-resolution (LR) samples\n",
        "print('Low-Resolution (LR) samples: ')\n",
        "plt.rcParams['figure.figsize'] = [14, 14]\n",
        "for image in train_lr_files[:n]:  # Loop through the first n LR images\n",
        "    ax = plt.subplot(2, n, i)  # Create subplot for the current image\n",
        "    plt.imshow(np.load(image).reshape(64,64), cmap='gray')  # Load and display the image in grayscale\n",
        "    ax.get_xaxis().set_visible(False)  # Hide x-axis\n",
        "    ax.get_yaxis().set_visible(False)  # Hide y-axis\n",
        "    i += 1  # Increment the subplot index\n",
        "plt.show()  # Display the plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYWK0ZkMIMSr"
      },
      "source": [
        "#### 1.3 Import Training and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yrkV-fO9mWI"
      },
      "outputs": [],
      "source": [
        "# Set Batch Size\n",
        "batch_size = 100\n",
        "\n",
        "# Define a custom Dataset class for loading Super Resolution data\n",
        "class SuperResolutionDataset(data.Dataset):\n",
        "    def __init__(self, lr_path, hr_path):\n",
        "        # Initialize the dataset with lists of low-resolution and high-resolution image file paths\n",
        "        self.lr_files = [os.path.join(lr_path, f) for f in os.listdir(lr_path) if f.endswith(\".npy\")]\n",
        "        self.hr_files = [os.path.join(hr_path, f) for f in os.listdir(hr_path) if f.endswith(\".npy\")]\n",
        "        \n",
        "    def __len__(self):\n",
        "        # Return the total number of low-resolution images (The number of HR and LR images is the same)\n",
        "        return len(self.lr_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load the low-resolution and high-resolution images from the file paths\n",
        "        lr_image = np.load(self.lr_files[idx])\n",
        "        hr_image = np.load(self.hr_files[idx])\n",
        "        # Convert numpy arrays to PyTorch tensors and return them\n",
        "        return torch.from_numpy(lr_image).float(), torch.from_numpy(hr_image).float()\n",
        "\n",
        "# Create the training data loader\n",
        "train_data = SuperResolutionDataset('./dataset_superres/train/LR', './dataset_superres/train/HR')\n",
        "train_data_loader = data.DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "# Create the validation data loader\n",
        "val_data = SuperResolutionDataset('./dataset_superres/val/LR', './dataset_superres/val/HR')\n",
        "val_data_loader = data.DataLoader(val_data, batch_size=batch_size, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABBx_F7oI_vC"
      },
      "source": [
        "### 2. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clDeWqesKU_8"
      },
      "source": [
        "#### 2.1 Defining a Super-Resolution CNN Model\n",
        "\n",
        "You may refer to this [article](https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148) to learn about Convolutional Neural Networks (CNN) and this [article](https://medium.com/coinmonks/review-srcnn-super-resolution-3cb3a4f67a7c) to learn more about how CNNs can be used for super-resolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlDjFP-mraJG"
      },
      "outputs": [],
      "source": [
        "# Define the Super-Resolution Convolutional Neural Network (SRCNN) model\n",
        "class SRCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SRCNN, self).__init__()\n",
        "        # First convolutional layer: 1 input channel, 64 output channels, 9x9 kernel, 4 pixels padding\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=9, padding=4)\n",
        "        # Second convolutional layer: 64 input channels, 32 output channels, 5x5 kernel, 2 pixels padding\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=5, padding=2)\n",
        "        # Third convolutional layer: 32 input channels, 1 output channel, 5x5 kernel, 2 pixels padding\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=1, kernel_size=5, padding=2)\n",
        "        # ReLU activation function\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the first convolutional layer followed by ReLU activation\n",
        "        x = self.relu(self.conv1(x))\n",
        "        # Apply the second convolutional layer followed by ReLU activation\n",
        "        x = self.relu(self.conv2(x))\n",
        "        # Apply the third convolutional layer\n",
        "        x = self.conv3(x)\n",
        "        # Upsample the output to the HR resolution using bicubic interpolation\n",
        "        x = F.interpolate(x, size=(128, 128), mode='bicubic', align_corners=False)\n",
        "        return x\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Instantiate the SRCNN model and move it to the appropriate device\n",
        "model = SRCNN().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ixy2T3PKpes"
      },
      "source": [
        "#### 2.2 Training the Super-Resolution CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dsVNm2J7K67"
      },
      "outputs": [],
      "source": [
        "# Loss Function\n",
        "criteria = nn.MSELoss()  # Mean Squared Error Loss\n",
        "\n",
        "# Optimizer (Adam)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)  \n",
        "\n",
        "# Train the model\n",
        "n_epochs = 20  # Number of Training Epochs\n",
        "loss_array = []  # Array to store the loss values\n",
        "pbar = tqdm(range(1, n_epochs+1))  # Progress bar for tracking epochs\n",
        "for epoch in pbar:\n",
        "    train_loss = 0.0  # Initialize training loss for the epoch\n",
        "\n",
        "    # Iterate over the training data loader\n",
        "    for step, (lr, hr) in enumerate(train_data_loader):\n",
        "\n",
        "        lr = Variable(lr).type(torch.FloatTensor).to(device)  # Move low-resolution images to the device\n",
        "        hr = Variable(hr).type(torch.FloatTensor).to(device)  # Move high-resolution images to the device\n",
        "        optimizer.zero_grad()  # Clear the gradients\n",
        "        outputs = model(lr)  # Forward pass through the model\n",
        "        loss = criteria(outputs, hr)  # Calculate the loss\n",
        "        loss.backward()  # Backpropagation\n",
        "        optimizer.step()  # Update the model parameters\n",
        "\n",
        "        train_loss += loss.item()  # Accumulate the training loss\n",
        "\n",
        "    train_loss = train_loss / len(train_data_loader)  # Compute average training loss for the epoch\n",
        "    loss_array.append(train_loss)  # Append the loss to the loss array\n",
        "    # Display the Training Stats\n",
        "    pbar.set_postfix({'Training Loss': train_loss})  # Update progress bar with training loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s73f62tkLopN"
      },
      "source": [
        "### 3. Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPIMrzmAMpsv"
      },
      "source": [
        "#### 3.1 Testing the Super-Resolution CNN Model on Validation Data - Calculate Quantitative Metrics\n",
        "\n",
        "- **MSE (Mean Squared Error):** A measure of the average squared difference between the estimated values and the actual value. Lower values indicate better performance.\n",
        "\n",
        "- **SSIM (Structural Similarity Index):** A method for measuring the similarity between two images. It is used to measure the quality of the super-resolved images compared to the original high-resolution images.\n",
        "\n",
        "- **PSNR (Peak Signal-to-Noise Ratio):** The ratio between the maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. Higher values indicate better image quality.\n",
        "\n",
        "You may refer to this [article](https://medium.com/@datamonsters/a-quick-overview-of-methods-to-measure-the-similarity-between-images-f907166694ee) to learn more about these metrics\n",
        "\n",
        "*Note: Metrics need to be calculated on a sample-by-sample basis, not on a batch basis. This is because metrics like SSIM and PSNR are used for assessing the quality of individual images and the scikit-image functions do not average over the first axis when we pass batches of images to them.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate Metrics\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "out = []  # List to store model predictions\n",
        "with torch.no_grad():  # Disable gradient calculation for validation\n",
        "    for lr, hr in val_data_loader:\n",
        "        lr = lr.to(device)  # Move low-resolution images to the device\n",
        "        hr = hr.to(device)  # Move high-resolution images to the device\n",
        "        recon = model(lr)  # Get model predictions\n",
        "        out.append(recon.cpu().detach().numpy())  # Append predictions to the list and move to CPU\n",
        "        del lr, hr, recon  # Free memory\n",
        "        torch.cuda.empty_cache()  # Clear cached memory\n",
        "dataSR = np.concatenate(out, axis=0)  # Concatenate predictions along the batch axis\n",
        "\n",
        "# Prepare ground truth for comparison\n",
        "val_hr = []\n",
        "for _, hr in val_data_loader:\n",
        "    val_hr.append(hr.cpu().numpy())  # Append ground truth high-resolution images to the list and move to CPU\n",
        "val_hr = np.concatenate(val_hr, axis=0)  # Concatenate ground truth images along the batch axis\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"Metrics:\")\n",
        "criteria = nn.MSELoss()  # Mean Squared Error Loss\n",
        "criteria2 = nn.L1Loss()  # L1 Loss\n",
        "\n",
        "losses = []  # List to store MSE losses\n",
        "losses2 = []  # List to store L1 losses\n",
        "Ssim = []  # List to store SSIM scores\n",
        "Psnr = []  # List to store PSNR scores\n",
        "\n",
        "for i in range(dataSR.shape[0]):\n",
        "    # Calculate MSE loss between predicted and ground truth images\n",
        "    losses.append(criteria(torch.from_numpy(dataSR[i]), torch.from_numpy(val_hr[i])))\n",
        "    # Calculate L1 loss between predicted and ground truth images\n",
        "    losses2.append(criteria2(torch.from_numpy(dataSR[i]), torch.from_numpy(val_hr[i])))\n",
        "    # Calculate SSIM score between predicted and ground truth images\n",
        "    Ssim.append(ssim(val_hr[i][0], dataSR[i][0], data_range=dataSR[i][0].max() - dataSR[i][0].min()))\n",
        "    # Calculate PSNR score between predicted and ground truth images\n",
        "    Psnr.append(psnr(val_hr[i][0], dataSR[i][0], data_range=dataSR[i][0].max() - dataSR[i][0].min()))\n",
        "\n",
        "# Print average metrics\n",
        "print(\"Average MSE super resolution samples: \" + str('%.7f' % np.average(losses)))\n",
        "print(\"Average L1 super resolution samples: \" + str('%.7f' % np.average(losses2)))\n",
        "print(\"Average SSIM super resolution samples: \" + str('%.5f' % np.average(Ssim)))\n",
        "print(\"Average PSNR super resolution samples: \" + str('%.5f' % np.average(Psnr)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### 3.2 Visualize Outputs for Qualitative Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emtSHxk6xsOG"
      },
      "outputs": [],
      "source": [
        "# Visualize Outputs\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    for lr, hr in val_data_loader:\n",
        "        lr = lr.to(device)  # Move low-resolution images to the device\n",
        "        hr = hr.to(device)  # Move high-resolution images to the device\n",
        "        output = model(lr)  # Get model predictions\n",
        "\n",
        "        lr = lr.cpu().numpy()  # Move low-resolution images to CPU and convert to numpy array\n",
        "        output = output.cpu().numpy()  # Move predicted images to CPU and convert to numpy array\n",
        "        hr = hr.cpu().numpy()  # Move high-resolution images to CPU and convert to numpy array\n",
        "\n",
        "        # Display the results\n",
        "        plt.figure(figsize=(12, 8))  # Set figure size\n",
        "        for i in range(5):  # Display first 5 images\n",
        "            plt.subplot(3, 5, i + 1)  # Create subplot for low-resolution image\n",
        "            plt.imshow(lr[i].reshape(64, 64), cmap='gray')  # Display low-resolution image in grayscale\n",
        "            plt.title('Low Res')  # Set title for low-resolution image\n",
        "            plt.axis('off')  # Hide axis\n",
        "            plt.subplot(3, 5, i + 6)  # Create subplot for high-resolution image\n",
        "            plt.imshow(hr[i].reshape(128, 128), cmap='gray')  # Display high-resolution image in grayscale\n",
        "            plt.title('High Res')  # Set title for high-resolution image\n",
        "            plt.axis('off')  # Hide axis\n",
        "            plt.subplot(3, 5, i + 11)  # Create subplot for output image\n",
        "            plt.imshow(output[i].reshape(128, 128), cmap='gray')  # Display predicted image in grayscale\n",
        "            plt.title('Output')  # Set title for predicted image\n",
        "            plt.axis('off')  # Hide axis\n",
        "        plt.show()  # Display the figure\n",
        "        break  # Break after first batch to visualize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHD8VEzyOuad"
      },
      "source": [
        "## Submission Guidelines\n",
        "\n",
        "* Fill out the pre- and post- hackathon surveys.\n",
        "* You are required to submit a Google Colab Jupyter Notebook (.ipynb and pdf) clearly showing your implementation along with the evaluation metrics (MSE, SSIM, and PSNR) for the validation data.\n",
        "* You must also submit the final trained model, including the model architecture and the trained weights ( For example: HDF5 file, .pb file, .pt file, etc. )\n",
        "* You can use this example notebook as a template for your work.\n",
        "\n",
        "> **_NOTE:_**  You are free to use any ML framework such as PyTorch, Keras, TensorFlow, etc."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "name": "Hackathon_Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
